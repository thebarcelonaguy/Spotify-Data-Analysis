{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Random Forest to perform Machine Learning\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 469281 entries, 0 to 469280\n",
      "Data columns (total 17 columns):\n",
      " #   Column            Non-Null Count   Dtype  \n",
      "---  ------            --------------   -----  \n",
      " 0   popularity        469281 non-null  int64  \n",
      " 1   duration_ms       469281 non-null  int64  \n",
      " 2   explicit          469281 non-null  int64  \n",
      " 3   danceability      469281 non-null  float64\n",
      " 4   energy            469281 non-null  float64\n",
      " 5   key               469281 non-null  int64  \n",
      " 6   loudness          469281 non-null  float64\n",
      " 7   mode              469281 non-null  int64  \n",
      " 8   speechiness       469281 non-null  float64\n",
      " 9   acousticness      469281 non-null  float64\n",
      " 10  instrumentalness  469281 non-null  float64\n",
      " 11  liveness          469281 non-null  float64\n",
      " 12  valence           469281 non-null  float64\n",
      " 13  tempo             469281 non-null  float64\n",
      " 14  time_signature    469281 non-null  int64  \n",
      " 15  num_artists       469281 non-null  int64  \n",
      " 16  year              469281 non-null  int64  \n",
      "dtypes: float64(9), int64(8)\n",
      "memory usage: 60.9 MB\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset into a pandas dataframe\n",
    "cleanedMusicData = pd.read_csv('../Data/tracks_cleaned.csv')\n",
    "cleanedMusicData.drop('release_date',axis=1,inplace=True)\n",
    "cleanedMusicData.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Key is already an estimated value instead of actual we will drop it from the frame. Additonally, we drop release_date due to the inconsisitencies in the data where release_months are only available for some of the data. Since we have already created a column called year in the data cleaning part, we are still able to use that and take advantage of the strong relation between year and popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary variables\n",
    "cleanedMusicData.drop(['key'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning and arrangement\n",
    "time_signature_df=pd.get_dummies(cleanedMusicData[\"time_signature\"])\n",
    "cleanedMusicData = pd.concat([cleanedMusicData,time_signature_df],axis=1)\n",
    "cleanedMusicData['mode'] = np.where(cleanedMusicData['mode']=='Major', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data modelling\n",
    "X= cleanedMusicData.loc[:,cleanedMusicData.columns !=\"popularity\"] # all the features accept popularity\n",
    "y = cleanedMusicData[\"popularity\"] # the popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 469281 entries, 0 to 469280\n",
      "Data columns (total 20 columns):\n",
      " #   Column            Non-Null Count   Dtype  \n",
      "---  ------            --------------   -----  \n",
      " 0   duration_ms       469281 non-null  int64  \n",
      " 1   explicit          469281 non-null  int64  \n",
      " 2   danceability      469281 non-null  float64\n",
      " 3   energy            469281 non-null  float64\n",
      " 4   loudness          469281 non-null  float64\n",
      " 5   mode              469281 non-null  int64  \n",
      " 6   speechiness       469281 non-null  float64\n",
      " 7   acousticness      469281 non-null  float64\n",
      " 8   instrumentalness  469281 non-null  float64\n",
      " 9   liveness          469281 non-null  float64\n",
      " 10  valence           469281 non-null  float64\n",
      " 11  tempo             469281 non-null  float64\n",
      " 12  time_signature    469281 non-null  int64  \n",
      " 13  num_artists       469281 non-null  int64  \n",
      " 14  year              469281 non-null  int64  \n",
      " 15  0                 469281 non-null  uint8  \n",
      " 16  1                 469281 non-null  uint8  \n",
      " 17  3                 469281 non-null  uint8  \n",
      " 18  4                 469281 non-null  uint8  \n",
      " 19  5                 469281 non-null  uint8  \n",
      "dtypes: float64(9), int64(6), uint8(5)\n",
      "memory usage: 55.9 MB\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate the data to training and testing\n",
    "X_train, X_test, y_train,y_test=train_test_split(X,y,test_size=0.2)\n",
    "\n",
    "# save as np.array\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train) \n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-34 {color: black;background-color: white;}#sk-container-id-34 pre{padding: 0;}#sk-container-id-34 div.sk-toggleable {background-color: white;}#sk-container-id-34 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-34 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-34 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-34 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-34 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-34 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-34 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-34 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-34 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-34 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-34 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-34 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-34 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-34 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-34 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-34 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-34 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-34 div.sk-item {position: relative;z-index: 1;}#sk-container-id-34 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-34 div.sk-item::before, #sk-container-id-34 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-34 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-34 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-34 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-34 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-34 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-34 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-34 div.sk-label-container {text-align: center;}#sk-container-id-34 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-34 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-34\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(max_depth=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-44\" type=\"checkbox\" checked><label for=\"sk-estimator-id-44\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_depth=10)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(max_depth=10)"
      ]
     },
     "execution_count": 533,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random forest regressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model_random_forest = RandomForestRegressor(n_estimators=100, max_depth=10,)\n",
    "model_random_forest.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Performance\n",
      "R^2: 0.49331159747127684\n",
      "MSE:  171.03817164957607\n",
      "RMSE: 13.07815627867996\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Set Performance\")\n",
    "print(\"R^2: \" + str(model_random_forest.score(X_train,y_train)))\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Predict on the train set\n",
    "y_train_pred = model_random_forest.predict(X_train)\n",
    "\n",
    "# Calculate MSE and RMSE on train set\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "\n",
    "rmse_train = np.sqrt(mse_train)\n",
    "print(\"MSE: \", mse_train)\n",
    "print(\"RMSE:\", rmse_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Performance\n",
      "R^2: 0.47486637863325565\n",
      "MSE:  176.6339797766907\n",
      "RMSE: 13.290371694451991\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Set Performance\")\n",
    "print(\"R^2: \" + str(model_random_forest.score(X_test,y_test)))\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Predict on the train set\n",
    "y_test_pred = model_random_forest.predict(X_test)\n",
    "\n",
    "# Calculate MSE and RMSE on train set\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "rmse_test = np.sqrt(mse_test)\n",
    "print(\"MSE: \", mse_test)\n",
    "print(\"RMSE:\", rmse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 13.271554001544486\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "y_pred = model_random_forest.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"RMSE:\", rmse)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  \n",
    "We have used an arbitrary max_depth and number of classfiers for our Random Forests model. Let's see and explore if we can fine tune the model to improve the performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Fine Tuning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping non-essential factors\n",
    "# data modelling\n",
    "\n",
    "refinedMusicData = pd.read_csv('../Data/tracks_cleaned.csv')\n",
    "refinedMusicData.drop('release_date',axis=1,inplace=True)\n",
    "\n",
    "x_top_6= refinedMusicData.loc[:,refinedMusicData.columns !=\"popularity\"] # all the features accept popularity\n",
    "y_top_6 = refinedMusicData[\"popularity\"] # the popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 469281 entries, 0 to 469280\n",
      "Data columns (total 16 columns):\n",
      " #   Column            Non-Null Count   Dtype  \n",
      "---  ------            --------------   -----  \n",
      " 0   duration_ms       469281 non-null  int64  \n",
      " 1   explicit          469281 non-null  int64  \n",
      " 2   danceability      469281 non-null  float64\n",
      " 3   energy            469281 non-null  float64\n",
      " 4   key               469281 non-null  int64  \n",
      " 5   loudness          469281 non-null  float64\n",
      " 6   mode              469281 non-null  int64  \n",
      " 7   speechiness       469281 non-null  float64\n",
      " 8   acousticness      469281 non-null  float64\n",
      " 9   instrumentalness  469281 non-null  float64\n",
      " 10  liveness          469281 non-null  float64\n",
      " 11  valence           469281 non-null  float64\n",
      " 12  tempo             469281 non-null  float64\n",
      " 13  time_signature    469281 non-null  int64  \n",
      " 14  num_artists       469281 non-null  int64  \n",
      " 15  year              469281 non-null  int64  \n",
      "dtypes: float64(9), int64(7)\n",
      "memory usage: 57.3 MB\n"
     ]
    }
   ],
   "source": [
    "x_top_6.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 469281 entries, 0 to 469280\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Non-Null Count   Dtype  \n",
      "---  ------        --------------   -----  \n",
      " 0   duration_ms   469281 non-null  int64  \n",
      " 1   explicit      469281 non-null  int64  \n",
      " 2   danceability  469281 non-null  float64\n",
      " 3   energy        469281 non-null  float64\n",
      " 4   loudness      469281 non-null  float64\n",
      " 5   acousticness  469281 non-null  float64\n",
      "dtypes: float64(4), int64(2)\n",
      "memory usage: 21.5 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4h/1j6hp57x2_bftnnq3v5hnhc80000gn/T/ipykernel_32821/641859377.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x_top_6.drop(['key','year', 'num_artists', 'mode', 'speechiness', 'liveness', 'valence', 'tempo', 'time_signature','instrumentalness'],axis=1, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "x_top_6.drop(['key','year', 'num_artists', 'mode', 'speechiness', 'liveness', 'valence', 'tempo', 'time_signature','instrumentalness'],axis=1, inplace=True)\n",
    "x_top_6.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     35632\n",
       "35     9755\n",
       "23     9710\n",
       "1      9630\n",
       "36     9484\n",
       "      ...  \n",
       "97        2\n",
       "93        2\n",
       "95        1\n",
       "98        1\n",
       "96        1\n",
       "Name: popularity, Length: 99, dtype: int64"
      ]
     },
     "execution_count": 561,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_top_6.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that there is a imbalance in data for the minority classes. We can upsample. This would reduce model bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed Indexes:\n",
      "(2769284, 6) (2769284,)\n"
     ]
    }
   ],
   "source": [
    "# Split the Dataset into Train and Test\n",
    "X_train_top_6, X_test_top_6, y_train_top_6, y_test_top_6 = train_test_split(x_top_6, y_top_6, test_size = 0.2)\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "ros = RandomOverSampler()\n",
    "X_train_ros, y_train_ros = ros.fit_resample(X_train_top_6,y_train_top_6)\n",
    "\n",
    "print('Removed Indexes:')\n",
    "print(X_train_ros.shape,y_train_ros.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39    28258\n",
       "76    28258\n",
       "78    28258\n",
       "70    28258\n",
       "68    28258\n",
       "      ...  \n",
       "29    28258\n",
       "5     28258\n",
       "17    28258\n",
       "11    28258\n",
       "95    28258\n",
       "Name: popularity, Length: 98, dtype: int64"
      ]
     },
     "execution_count": 563,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_ros.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset is large. To reduce grid search time, we can use a subset of the data\n",
    "\n",
    "# Split the Dataset into Train and Test\n",
    "X_train_top_6_CV, X_test_top_6_CV, y_train_top_6_CV, y_test_top_6_CV = train_test_split(X_train_ros, y_train_ros, test_size = 0.995 ,stratify=y_train_ros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40    142\n",
       "15    142\n",
       "68    142\n",
       "94    142\n",
       "72    142\n",
       "     ... \n",
       "11    141\n",
       "35    141\n",
       "91    141\n",
       "54    141\n",
       "36    141\n",
       "Name: popularity, Length: 98, dtype: int64"
      ]
     },
     "execution_count": 574,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_top_6_CV.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 13846 entries, 2470266 to 2619945\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   duration_ms   13846 non-null  int64  \n",
      " 1   explicit      13846 non-null  int64  \n",
      " 2   danceability  13846 non-null  float64\n",
      " 3   energy        13846 non-null  float64\n",
      " 4   loudness      13846 non-null  float64\n",
      " 5   acousticness  13846 non-null  float64\n",
      "dtypes: float64(4), int64(2)\n",
      "memory usage: 757.2 KB\n"
     ]
    }
   ],
   "source": [
    "X_train_top_6_CV.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "[CV 1/5; 1/30] START max_depth=10, n_estimators=100.............................\n",
      "[CV 2/5; 1/30] START max_depth=10, n_estimators=100.............................\n",
      "[CV 3/5; 1/30] START max_depth=10, n_estimators=100.............................\n",
      "[CV 4/5; 1/30] START max_depth=10, n_estimators=100.............................\n",
      "[CV 5/5; 1/30] START max_depth=10, n_estimators=100.............................\n",
      "[CV 1/5; 2/30] START max_depth=10, n_estimators=200.............................\n",
      "[CV 2/5; 2/30] START max_depth=10, n_estimators=200.............................\n",
      "[CV 3/5; 2/30] START max_depth=10, n_estimators=200.............................\n",
      "[CV 2/5; 1/30] END max_depth=10, n_estimators=100;, score=0.552 total time=   3.3s\n",
      "[CV 4/5; 2/30] START max_depth=10, n_estimators=200.............................\n",
      "[CV 4/5; 1/30] END max_depth=10, n_estimators=100;, score=0.551 total time=   3.4s\n",
      "[CV 5/5; 1/30] END max_depth=10, n_estimators=100;, score=0.569 total time=   3.3s\n",
      "[CV 5/5; 2/30] START max_depth=10, n_estimators=200.............................\n",
      "[CV 1/5; 3/30] START max_depth=10, n_estimators=300.............................\n",
      "[CV 3/5; 1/30] END max_depth=10, n_estimators=100;, score=0.574 total time=   3.4s\n",
      "[CV 2/5; 3/30] START max_depth=10, n_estimators=300.............................\n",
      "[CV 1/5; 1/30] END max_depth=10, n_estimators=100;, score=0.558 total time=   3.5s\n",
      "[CV 3/5; 3/30] START max_depth=10, n_estimators=300.............................\n",
      "[CV 2/5; 2/30] END max_depth=10, n_estimators=200;, score=0.550 total time=   6.2s\n",
      "[CV 4/5; 3/30] START max_depth=10, n_estimators=300.............................\n",
      "[CV 3/5; 2/30] END max_depth=10, n_estimators=200;, score=0.572 total time=   6.6s\n",
      "[CV 5/5; 3/30] START max_depth=10, n_estimators=300.............................\n",
      "[CV 1/5; 2/30] END max_depth=10, n_estimators=200;, score=0.558 total time=   6.8s\n",
      "[CV 1/5; 4/30] START max_depth=10, n_estimators=400.............................\n",
      "[CV 4/5; 2/30] END max_depth=10, n_estimators=200;, score=0.552 total time=   6.3s\n",
      "[CV 2/5; 4/30] START max_depth=10, n_estimators=400.............................\n",
      "[CV 5/5; 2/30] END max_depth=10, n_estimators=200;, score=0.570 total time=   6.3s\n",
      "[CV 3/5; 4/30] START max_depth=10, n_estimators=400.............................\n",
      "[CV 1/5; 3/30] END max_depth=10, n_estimators=300;, score=0.561 total time=   9.6s\n",
      "[CV 4/5; 4/30] START max_depth=10, n_estimators=400.............................\n",
      "[CV 2/5; 3/30] END max_depth=10, n_estimators=300;, score=0.550 total time=   9.7s\n",
      "[CV 5/5; 4/30] START max_depth=10, n_estimators=400.............................\n",
      "[CV 3/5; 3/30] END max_depth=10, n_estimators=300;, score=0.573 total time=  10.1s\n",
      "[CV 1/5; 5/30] START max_depth=10, n_estimators=500.............................\n",
      "[CV 4/5; 3/30] END max_depth=10, n_estimators=300;, score=0.551 total time=   9.8s\n",
      "[CV 2/5; 5/30] START max_depth=10, n_estimators=500.............................\n",
      "[CV 5/5; 3/30] END max_depth=10, n_estimators=300;, score=0.572 total time=   9.8s\n",
      "[CV 3/5; 5/30] START max_depth=10, n_estimators=500.............................\n",
      "[CV 1/5; 4/30] END max_depth=10, n_estimators=400;, score=0.560 total time=  12.7s\n",
      "[CV 4/5; 5/30] START max_depth=10, n_estimators=500.............................\n",
      "[CV 3/5; 4/30] END max_depth=10, n_estimators=400;, score=0.574 total time=  13.0s\n",
      "[CV 5/5; 5/30] START max_depth=10, n_estimators=500.............................\n",
      "[CV 2/5; 4/30] END max_depth=10, n_estimators=400;, score=0.551 total time=  13.4s\n",
      "[CV 1/5; 6/30] START max_depth=10, n_estimators=600.............................\n",
      "[CV 5/5; 4/30] END max_depth=10, n_estimators=400;, score=0.571 total time=  13.2s\n",
      "[CV 2/5; 6/30] START max_depth=10, n_estimators=600.............................\n",
      "[CV 4/5; 4/30] END max_depth=10, n_estimators=400;, score=0.553 total time=  13.6s\n",
      "[CV 3/5; 6/30] START max_depth=10, n_estimators=600.............................\n",
      "[CV 1/5; 5/30] END max_depth=10, n_estimators=500;, score=0.560 total time=  16.0s\n",
      "[CV 4/5; 6/30] START max_depth=10, n_estimators=600.............................\n",
      "[CV 2/5; 5/30] END max_depth=10, n_estimators=500;, score=0.551 total time=  16.7s\n",
      "[CV 5/5; 6/30] START max_depth=10, n_estimators=600.............................\n",
      "[CV 3/5; 5/30] END max_depth=10, n_estimators=500;, score=0.573 total time=  16.8s\n",
      "[CV 1/5; 7/30] START max_depth=10, n_estimators=700.............................\n",
      "[CV 4/5; 5/30] END max_depth=10, n_estimators=500;, score=0.552 total time=  17.1s\n",
      "[CV 2/5; 7/30] START max_depth=10, n_estimators=700.............................\n",
      "[CV 5/5; 5/30] END max_depth=10, n_estimators=500;, score=0.572 total time=  17.3s\n",
      "[CV 3/5; 7/30] START max_depth=10, n_estimators=700.............................\n",
      "[CV 1/5; 6/30] END max_depth=10, n_estimators=600;, score=0.560 total time=  20.8s\n",
      "[CV 4/5; 7/30] START max_depth=10, n_estimators=700.............................\n",
      "[CV 2/5; 6/30] END max_depth=10, n_estimators=600;, score=0.549 total time=  21.1s\n",
      "[CV 5/5; 7/30] START max_depth=10, n_estimators=700.............................\n",
      "[CV 3/5; 6/30] END max_depth=10, n_estimators=600;, score=0.574 total time=  21.0s\n",
      "[CV 1/5; 8/30] START max_depth=10, n_estimators=800.............................\n",
      "[CV 4/5; 6/30] END max_depth=10, n_estimators=600;, score=0.552 total time=  21.6s\n",
      "[CV 2/5; 8/30] START max_depth=10, n_estimators=800.............................\n",
      "[CV 5/5; 6/30] END max_depth=10, n_estimators=600;, score=0.572 total time=  21.7s\n",
      "[CV 3/5; 8/30] START max_depth=10, n_estimators=800.............................\n",
      "[CV 1/5; 7/30] END max_depth=10, n_estimators=700;, score=0.560 total time=  24.7s\n",
      "[CV 4/5; 8/30] START max_depth=10, n_estimators=800.............................\n",
      "[CV 2/5; 7/30] END max_depth=10, n_estimators=700;, score=0.551 total time=  25.3s\n",
      "[CV 5/5; 8/30] START max_depth=10, n_estimators=800.............................\n",
      "[CV 3/5; 7/30] END max_depth=10, n_estimators=700;, score=0.573 total time=  25.1s\n",
      "[CV 1/5; 9/30] START max_depth=10, n_estimators=900.............................\n",
      "[CV 4/5; 7/30] END max_depth=10, n_estimators=700;, score=0.552 total time=  25.1s\n",
      "[CV 2/5; 9/30] START max_depth=10, n_estimators=900.............................\n",
      "[CV 5/5; 7/30] END max_depth=10, n_estimators=700;, score=0.572 total time=  24.8s\n",
      "[CV 3/5; 9/30] START max_depth=10, n_estimators=900.............................\n",
      "[CV 1/5; 8/30] END max_depth=10, n_estimators=800;, score=0.561 total time=  28.0s\n",
      "[CV 4/5; 9/30] START max_depth=10, n_estimators=900.............................\n",
      "[CV 2/5; 8/30] END max_depth=10, n_estimators=800;, score=0.550 total time=  28.5s\n",
      "[CV 5/5; 9/30] START max_depth=10, n_estimators=900.............................\n",
      "[CV 3/5; 8/30] END max_depth=10, n_estimators=800;, score=0.574 total time=  27.9s\n",
      "[CV 1/5; 10/30] START max_depth=10, n_estimators=1000...........................\n",
      "[CV 4/5; 8/30] END max_depth=10, n_estimators=800;, score=0.552 total time=  28.1s\n",
      "[CV 2/5; 10/30] START max_depth=10, n_estimators=1000...........................\n",
      "[CV 5/5; 8/30] END max_depth=10, n_estimators=800;, score=0.572 total time=  28.0s\n",
      "[CV 3/5; 10/30] START max_depth=10, n_estimators=1000...........................\n",
      "[CV 1/5; 9/30] END max_depth=10, n_estimators=900;, score=0.561 total time=  31.5s\n",
      "[CV 4/5; 10/30] START max_depth=10, n_estimators=1000...........................\n",
      "[CV 2/5; 9/30] END max_depth=10, n_estimators=900;, score=0.551 total time=  32.4s\n",
      "[CV 5/5; 10/30] START max_depth=10, n_estimators=1000...........................\n",
      "[CV 3/5; 9/30] END max_depth=10, n_estimators=900;, score=0.573 total time=  32.0s\n",
      "[CV 1/5; 11/30] START max_depth=12, n_estimators=100............................\n",
      "[CV 4/5; 9/30] END max_depth=10, n_estimators=900;, score=0.552 total time=  32.6s\n",
      "[CV 2/5; 11/30] START max_depth=12, n_estimators=100............................\n",
      "[CV 1/5; 11/30] END max_depth=12, n_estimators=100;, score=0.591 total time=   4.4s\n",
      "[CV 3/5; 11/30] START max_depth=12, n_estimators=100............................\n",
      "[CV 2/5; 11/30] END max_depth=12, n_estimators=100;, score=0.580 total time=   4.3s\n",
      "[CV 4/5; 11/30] START max_depth=12, n_estimators=100............................\n",
      "[CV 5/5; 9/30] END max_depth=10, n_estimators=900;, score=0.572 total time=  33.3s\n",
      "[CV 5/5; 11/30] START max_depth=12, n_estimators=100............................\n",
      "[CV 3/5; 11/30] END max_depth=12, n_estimators=100;, score=0.597 total time=   4.4s\n",
      "[CV 1/5; 12/30] START max_depth=12, n_estimators=200............................\n",
      "[CV 4/5; 11/30] END max_depth=12, n_estimators=100;, score=0.578 total time=   4.3s\n",
      "[CV 2/5; 12/30] START max_depth=12, n_estimators=200............................\n",
      "[CV 5/5; 11/30] END max_depth=12, n_estimators=100;, score=0.595 total time=   4.2s\n",
      "[CV 3/5; 12/30] START max_depth=12, n_estimators=200............................\n",
      "[CV 1/5; 10/30] END max_depth=10, n_estimators=1000;, score=0.561 total time=  37.0s\n",
      "[CV 4/5; 12/30] START max_depth=12, n_estimators=200............................\n",
      "[CV 1/5; 12/30] END max_depth=12, n_estimators=200;, score=0.590 total time=   8.6s\n",
      "[CV 5/5; 12/30] START max_depth=12, n_estimators=200............................\n",
      "[CV 2/5; 10/30] END max_depth=10, n_estimators=1000;, score=0.550 total time=  37.0s\n",
      "[CV 1/5; 13/30] START max_depth=12, n_estimators=300............................\n",
      "[CV 2/5; 12/30] END max_depth=12, n_estimators=200;, score=0.581 total time=   8.7s\n",
      "[CV 2/5; 13/30] START max_depth=12, n_estimators=300............................\n",
      "[CV 3/5; 12/30] END max_depth=12, n_estimators=200;, score=0.596 total time=   8.6s\n",
      "[CV 3/5; 13/30] START max_depth=12, n_estimators=300............................\n",
      "[CV 3/5; 10/30] END max_depth=10, n_estimators=1000;, score=0.574 total time=  37.8s\n",
      "[CV 4/5; 13/30] START max_depth=12, n_estimators=300............................\n",
      "[CV 4/5; 12/30] END max_depth=12, n_estimators=200;, score=0.577 total time=   8.6s\n",
      "[CV 5/5; 13/30] START max_depth=12, n_estimators=300............................\n",
      "[CV 5/5; 12/30] END max_depth=12, n_estimators=200;, score=0.595 total time=   8.6s\n",
      "[CV 1/5; 14/30] START max_depth=12, n_estimators=400............................\n",
      "[CV 4/5; 10/30] END max_depth=10, n_estimators=1000;, score=0.553 total time=  37.1s\n",
      "[CV 2/5; 14/30] START max_depth=12, n_estimators=400............................\n",
      "[CV 1/5; 13/30] END max_depth=12, n_estimators=300;, score=0.591 total time=  12.8s\n",
      "[CV 3/5; 14/30] START max_depth=12, n_estimators=400............................\n",
      "[CV 2/5; 13/30] END max_depth=12, n_estimators=300;, score=0.582 total time=  12.6s\n",
      "[CV 4/5; 14/30] START max_depth=12, n_estimators=400............................\n",
      "[CV 3/5; 13/30] END max_depth=12, n_estimators=300;, score=0.598 total time=  12.3s\n",
      "[CV 5/5; 14/30] START max_depth=12, n_estimators=400............................\n",
      "[CV 5/5; 10/30] END max_depth=10, n_estimators=1000;, score=0.571 total time=  38.3s\n",
      "[CV 1/5; 15/30] START max_depth=12, n_estimators=500............................\n",
      "[CV 4/5; 13/30] END max_depth=12, n_estimators=300;, score=0.578 total time=  12.9s\n",
      "[CV 2/5; 15/30] START max_depth=12, n_estimators=500............................\n",
      "[CV 5/5; 13/30] END max_depth=12, n_estimators=300;, score=0.594 total time=  12.9s\n",
      "[CV 3/5; 15/30] START max_depth=12, n_estimators=500............................\n",
      "[CV 1/5; 14/30] END max_depth=12, n_estimators=400;, score=0.592 total time=  16.7s\n",
      "[CV 4/5; 15/30] START max_depth=12, n_estimators=500............................\n",
      "[CV 2/5; 14/30] END max_depth=12, n_estimators=400;, score=0.581 total time=  16.3s\n",
      "[CV 5/5; 15/30] START max_depth=12, n_estimators=500............................\n",
      "[CV 3/5; 14/30] END max_depth=12, n_estimators=400;, score=0.598 total time=  16.8s\n",
      "[CV 1/5; 16/30] START max_depth=12, n_estimators=600............................\n",
      "[CV 5/5; 14/30] END max_depth=12, n_estimators=400;, score=0.594 total time=  16.2s\n",
      "[CV 2/5; 16/30] START max_depth=12, n_estimators=600............................\n",
      "[CV 4/5; 14/30] END max_depth=12, n_estimators=400;, score=0.577 total time=  16.7s\n",
      "[CV 3/5; 16/30] START max_depth=12, n_estimators=600............................\n",
      "[CV 1/5; 15/30] END max_depth=12, n_estimators=500;, score=0.593 total time=  20.9s\n",
      "[CV 4/5; 16/30] START max_depth=12, n_estimators=600............................\n",
      "[CV 2/5; 15/30] END max_depth=12, n_estimators=500;, score=0.582 total time=  20.9s\n",
      "[CV 5/5; 16/30] START max_depth=12, n_estimators=600............................\n",
      "[CV 3/5; 15/30] END max_depth=12, n_estimators=500;, score=0.598 total time=  20.7s\n",
      "[CV 1/5; 17/30] START max_depth=12, n_estimators=700............................\n",
      "[CV 4/5; 15/30] END max_depth=12, n_estimators=500;, score=0.578 total time=  20.9s\n",
      "[CV 2/5; 17/30] START max_depth=12, n_estimators=700............................\n",
      "[CV 5/5; 15/30] END max_depth=12, n_estimators=500;, score=0.595 total time=  21.1s\n",
      "[CV 3/5; 17/30] START max_depth=12, n_estimators=700............................\n",
      "[CV 1/5; 16/30] END max_depth=12, n_estimators=600;, score=0.592 total time=  25.0s\n",
      "[CV 4/5; 17/30] START max_depth=12, n_estimators=700............................\n",
      "[CV 2/5; 16/30] END max_depth=12, n_estimators=600;, score=0.583 total time=  25.2s\n",
      "[CV 5/5; 17/30] START max_depth=12, n_estimators=700............................\n",
      "[CV 3/5; 16/30] END max_depth=12, n_estimators=600;, score=0.598 total time=  25.3s\n",
      "[CV 1/5; 18/30] START max_depth=12, n_estimators=800............................\n",
      "[CV 4/5; 16/30] END max_depth=12, n_estimators=600;, score=0.578 total time=  25.2s\n",
      "[CV 2/5; 18/30] START max_depth=12, n_estimators=800............................\n",
      "[CV 5/5; 16/30] END max_depth=12, n_estimators=600;, score=0.595 total time=  25.3s\n",
      "[CV 3/5; 18/30] START max_depth=12, n_estimators=800............................\n",
      "[CV 1/5; 17/30] END max_depth=12, n_estimators=700;, score=0.593 total time=  29.0s\n",
      "[CV 4/5; 18/30] START max_depth=12, n_estimators=800............................\n",
      "[CV 2/5; 17/30] END max_depth=12, n_estimators=700;, score=0.582 total time=  28.1s\n",
      "[CV 5/5; 18/30] START max_depth=12, n_estimators=800............................\n",
      "[CV 3/5; 17/30] END max_depth=12, n_estimators=700;, score=0.598 total time=  29.0s\n",
      "[CV 1/5; 19/30] START max_depth=12, n_estimators=900............................\n",
      "[CV 4/5; 17/30] END max_depth=12, n_estimators=700;, score=0.578 total time=  29.2s\n",
      "[CV 2/5; 19/30] START max_depth=12, n_estimators=900............................\n",
      "[CV 5/5; 17/30] END max_depth=12, n_estimators=700;, score=0.595 total time=  30.2s\n",
      "[CV 3/5; 19/30] START max_depth=12, n_estimators=900............................\n",
      "[CV 1/5; 18/30] END max_depth=12, n_estimators=800;, score=0.593 total time=  34.5s\n",
      "[CV 4/5; 19/30] START max_depth=12, n_estimators=900............................\n",
      "[CV 2/5; 18/30] END max_depth=12, n_estimators=800;, score=0.582 total time=  33.5s\n",
      "[CV 5/5; 19/30] START max_depth=12, n_estimators=900............................\n",
      "[CV 3/5; 18/30] END max_depth=12, n_estimators=800;, score=0.598 total time=  33.2s\n",
      "[CV 1/5; 20/30] START max_depth=12, n_estimators=1000...........................\n",
      "[CV 4/5; 18/30] END max_depth=12, n_estimators=800;, score=0.579 total time=  33.5s\n",
      "[CV 2/5; 20/30] START max_depth=12, n_estimators=1000...........................\n",
      "[CV 5/5; 18/30] END max_depth=12, n_estimators=800;, score=0.596 total time=  33.4s\n",
      "[CV 3/5; 20/30] START max_depth=12, n_estimators=1000...........................\n",
      "[CV 1/5; 19/30] END max_depth=12, n_estimators=900;, score=0.592 total time=  37.6s\n",
      "[CV 4/5; 20/30] START max_depth=12, n_estimators=1000...........................\n",
      "[CV 2/5; 19/30] END max_depth=12, n_estimators=900;, score=0.583 total time=  37.9s\n",
      "[CV 5/5; 20/30] START max_depth=12, n_estimators=1000...........................\n",
      "[CV 3/5; 19/30] END max_depth=12, n_estimators=900;, score=0.599 total time=  37.4s\n",
      "[CV 1/5; 21/30] START max_depth=15, n_estimators=100............................\n",
      "[CV 1/5; 21/30] END max_depth=15, n_estimators=100;, score=0.618 total time=   5.1s\n",
      "[CV 2/5; 21/30] START max_depth=15, n_estimators=100............................\n",
      "[CV 4/5; 19/30] END max_depth=12, n_estimators=900;, score=0.579 total time=  37.9s\n",
      "[CV 3/5; 21/30] START max_depth=15, n_estimators=100............................\n",
      "[CV 2/5; 21/30] END max_depth=15, n_estimators=100;, score=0.608 total time=   5.0s\n",
      "[CV 4/5; 21/30] START max_depth=15, n_estimators=100............................\n",
      "[CV 5/5; 19/30] END max_depth=12, n_estimators=900;, score=0.596 total time=  38.2s\n",
      "[CV 5/5; 21/30] START max_depth=15, n_estimators=100............................\n",
      "[CV 3/5; 21/30] END max_depth=15, n_estimators=100;, score=0.615 total time=   5.1s\n",
      "[CV 1/5; 22/30] START max_depth=15, n_estimators=200............................\n",
      "[CV 5/5; 21/30] END max_depth=15, n_estimators=100;, score=0.610 total time=   4.7s\n",
      "[CV 2/5; 22/30] START max_depth=15, n_estimators=200............................\n",
      "[CV 4/5; 21/30] END max_depth=15, n_estimators=100;, score=0.598 total time=   5.0s\n",
      "[CV 3/5; 22/30] START max_depth=15, n_estimators=200............................\n",
      "[CV 1/5; 20/30] END max_depth=12, n_estimators=1000;, score=0.593 total time=  42.6s\n",
      "[CV 4/5; 22/30] START max_depth=15, n_estimators=200............................\n",
      "[CV 2/5; 20/30] END max_depth=12, n_estimators=1000;, score=0.583 total time=  43.0s\n",
      "[CV 5/5; 22/30] START max_depth=15, n_estimators=200............................\n",
      "[CV 1/5; 22/30] END max_depth=15, n_estimators=200;, score=0.617 total time=  10.1s\n",
      "[CV 1/5; 23/30] START max_depth=15, n_estimators=300............................\n",
      "[CV 2/5; 22/30] END max_depth=15, n_estimators=200;, score=0.607 total time=   9.7s\n",
      "[CV 2/5; 23/30] START max_depth=15, n_estimators=300............................\n",
      "[CV 3/5; 22/30] END max_depth=15, n_estimators=200;, score=0.616 total time=   9.7s\n",
      "[CV 3/5; 23/30] START max_depth=15, n_estimators=300............................\n",
      "[CV 4/5; 22/30] END max_depth=15, n_estimators=200;, score=0.599 total time=   9.6s\n",
      "[CV 4/5; 23/30] START max_depth=15, n_estimators=300............................\n",
      "[CV 3/5; 20/30] END max_depth=12, n_estimators=1000;, score=0.598 total time=  43.0s\n",
      "[CV 5/5; 23/30] START max_depth=15, n_estimators=300............................\n",
      "[CV 5/5; 22/30] END max_depth=15, n_estimators=200;, score=0.612 total time=  10.4s\n",
      "[CV 1/5; 24/30] START max_depth=15, n_estimators=400............................\n",
      "[CV 4/5; 20/30] END max_depth=12, n_estimators=1000;, score=0.578 total time=  44.0s\n",
      "[CV 2/5; 24/30] START max_depth=15, n_estimators=400............................\n",
      "[CV 1/5; 23/30] END max_depth=15, n_estimators=300;, score=0.619 total time=  16.4s\n",
      "[CV 3/5; 24/30] START max_depth=15, n_estimators=400............................\n",
      "[CV 2/5; 23/30] END max_depth=15, n_estimators=300;, score=0.608 total time=  15.9s\n",
      "[CV 3/5; 23/30] END max_depth=15, n_estimators=300;, score=0.616 total time=  15.7s\n",
      "[CV 4/5; 24/30] START max_depth=15, n_estimators=400............................\n",
      "[CV 5/5; 24/30] START max_depth=15, n_estimators=400............................\n",
      "[CV 5/5; 23/30] END max_depth=15, n_estimators=300;, score=0.611 total time=  16.2s\n",
      "[CV 1/5; 25/30] START max_depth=15, n_estimators=500............................\n",
      "[CV 4/5; 23/30] END max_depth=15, n_estimators=300;, score=0.600 total time=  16.4s\n",
      "[CV 2/5; 25/30] START max_depth=15, n_estimators=500............................\n",
      "[CV 5/5; 20/30] END max_depth=12, n_estimators=1000;, score=0.596 total time=  43.9s\n",
      "[CV 3/5; 25/30] START max_depth=15, n_estimators=500............................\n",
      "[CV 1/5; 24/30] END max_depth=15, n_estimators=400;, score=0.619 total time=  20.5s\n",
      "[CV 4/5; 25/30] START max_depth=15, n_estimators=500............................\n",
      "[CV 2/5; 24/30] END max_depth=15, n_estimators=400;, score=0.608 total time=  20.6s\n",
      "[CV 5/5; 25/30] START max_depth=15, n_estimators=500............................\n",
      "[CV 3/5; 24/30] END max_depth=15, n_estimators=400;, score=0.616 total time=  20.5s\n",
      "[CV 1/5; 26/30] START max_depth=15, n_estimators=600............................\n",
      "[CV 4/5; 24/30] END max_depth=15, n_estimators=400;, score=0.600 total time=  20.4s\n",
      "[CV 2/5; 26/30] START max_depth=15, n_estimators=600............................\n",
      "[CV 5/5; 24/30] END max_depth=15, n_estimators=400;, score=0.613 total time=  20.4s\n",
      "[CV 3/5; 26/30] START max_depth=15, n_estimators=600............................\n",
      "[CV 3/5; 25/30] END max_depth=15, n_estimators=500;, score=0.616 total time=  25.1s\n",
      "[CV 4/5; 26/30] START max_depth=15, n_estimators=600............................\n",
      "[CV 2/5; 25/30] END max_depth=15, n_estimators=500;, score=0.607 total time=  25.5s\n",
      "[CV 5/5; 26/30] START max_depth=15, n_estimators=600............................\n",
      "[CV 1/5; 25/30] END max_depth=15, n_estimators=500;, score=0.619 total time=  25.6s\n",
      "[CV 1/5; 27/30] START max_depth=15, n_estimators=700............................\n",
      "[CV 4/5; 25/30] END max_depth=15, n_estimators=500;, score=0.601 total time=  25.6s\n",
      "[CV 2/5; 27/30] START max_depth=15, n_estimators=700............................\n",
      "[CV 5/5; 25/30] END max_depth=15, n_estimators=500;, score=0.612 total time=  25.6s\n",
      "[CV 3/5; 27/30] START max_depth=15, n_estimators=700............................\n",
      "[CV 1/5; 26/30] END max_depth=15, n_estimators=600;, score=0.618 total time=  31.7s\n",
      "[CV 4/5; 27/30] START max_depth=15, n_estimators=700............................\n",
      "[CV 3/5; 26/30] END max_depth=15, n_estimators=600;, score=0.617 total time=  30.7s\n",
      "[CV 5/5; 27/30] START max_depth=15, n_estimators=700............................\n",
      "[CV 2/5; 26/30] END max_depth=15, n_estimators=600;, score=0.608 total time=  31.1s\n",
      "[CV 1/5; 28/30] START max_depth=15, n_estimators=800............................\n",
      "[CV 5/5; 26/30] END max_depth=15, n_estimators=600;, score=0.613 total time=  30.5s\n",
      "[CV 2/5; 28/30] START max_depth=15, n_estimators=800............................\n",
      "[CV 4/5; 26/30] END max_depth=15, n_estimators=600;, score=0.600 total time=  30.9s\n",
      "[CV 3/5; 28/30] START max_depth=15, n_estimators=800............................\n",
      "[CV 1/5; 27/30] END max_depth=15, n_estimators=700;, score=0.620 total time=  35.4s\n",
      "[CV 4/5; 28/30] START max_depth=15, n_estimators=800............................\n",
      "[CV 2/5; 27/30] END max_depth=15, n_estimators=700;, score=0.608 total time=  34.8s\n",
      "[CV 5/5; 28/30] START max_depth=15, n_estimators=800............................\n",
      "[CV 3/5; 27/30] END max_depth=15, n_estimators=700;, score=0.616 total time=  34.7s\n",
      "[CV 1/5; 29/30] START max_depth=15, n_estimators=900............................\n",
      "[CV 4/5; 27/30] END max_depth=15, n_estimators=700;, score=0.600 total time=  34.4s\n",
      "[CV 2/5; 29/30] START max_depth=15, n_estimators=900............................\n",
      "[CV 5/5; 27/30] END max_depth=15, n_estimators=700;, score=0.612 total time=  34.6s\n",
      "[CV 3/5; 29/30] START max_depth=15, n_estimators=900............................\n",
      "[CV 1/5; 28/30] END max_depth=15, n_estimators=800;, score=0.620 total time=  39.1s\n",
      "[CV 4/5; 29/30] START max_depth=15, n_estimators=900............................\n",
      "[CV 2/5; 28/30] END max_depth=15, n_estimators=800;, score=0.608 total time=  39.0s\n",
      "[CV 5/5; 29/30] START max_depth=15, n_estimators=900............................\n",
      "[CV 3/5; 28/30] END max_depth=15, n_estimators=800;, score=0.616 total time=  38.9s\n",
      "[CV 1/5; 30/30] START max_depth=15, n_estimators=1000...........................\n",
      "[CV 4/5; 28/30] END max_depth=15, n_estimators=800;, score=0.599 total time=  39.2s\n",
      "[CV 2/5; 30/30] START max_depth=15, n_estimators=1000...........................\n",
      "[CV 5/5; 28/30] END max_depth=15, n_estimators=800;, score=0.612 total time=  39.2s\n",
      "[CV 3/5; 30/30] START max_depth=15, n_estimators=1000...........................\n",
      "[CV 1/5; 29/30] END max_depth=15, n_estimators=900;, score=0.619 total time=  43.2s\n",
      "[CV 4/5; 30/30] START max_depth=15, n_estimators=1000...........................\n",
      "[CV 2/5; 29/30] END max_depth=15, n_estimators=900;, score=0.608 total time=  45.2s\n",
      "[CV 5/5; 30/30] START max_depth=15, n_estimators=1000...........................\n",
      "[CV 3/5; 29/30] END max_depth=15, n_estimators=900;, score=0.617 total time=  44.8s\n",
      "[CV 4/5; 29/30] END max_depth=15, n_estimators=900;, score=0.600 total time=  44.6s\n",
      "[CV 5/5; 29/30] END max_depth=15, n_estimators=900;, score=0.612 total time=  45.7s\n",
      "[CV 1/5; 30/30] END max_depth=15, n_estimators=1000;, score=0.620 total time=  48.6s\n",
      "[CV 2/5; 30/30] END max_depth=15, n_estimators=1000;, score=0.609 total time=  46.4s\n",
      "[CV 3/5; 30/30] END max_depth=15, n_estimators=1000;, score=0.617 total time=  43.6s\n",
      "[CV 4/5; 30/30] END max_depth=15, n_estimators=1000;, score=0.600 total time=  40.2s\n",
      "[CV 5/5; 30/30] END max_depth=15, n_estimators=1000;, score=0.612 total time=  35.6s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-37 {color: black;background-color: white;}#sk-container-id-37 pre{padding: 0;}#sk-container-id-37 div.sk-toggleable {background-color: white;}#sk-container-id-37 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-37 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-37 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-37 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-37 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-37 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-37 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-37 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-37 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-37 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-37 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-37 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-37 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-37 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-37 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-37 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-37 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-37 div.sk-item {position: relative;z-index: 1;}#sk-container-id-37 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-37 div.sk-item::before, #sk-container-id-37 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-37 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-37 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-37 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-37 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-37 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-37 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-37 div.sk-label-container {text-align: center;}#sk-container-id-37 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-37 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-37\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=RandomForestRegressor(), n_jobs=-1,\n",
       "             param_grid={&#x27;max_depth&#x27;: [10, 12, 15],\n",
       "                         &#x27;n_estimators&#x27;: array([ 100,  200,  300,  400,  500,  600,  700,  800,  900, 1000])},\n",
       "             scoring=&#x27;r2&#x27;, verbose=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-51\" type=\"checkbox\" ><label for=\"sk-estimator-id-51\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=RandomForestRegressor(), n_jobs=-1,\n",
       "             param_grid={&#x27;max_depth&#x27;: [10, 12, 15],\n",
       "                         &#x27;n_estimators&#x27;: array([ 100,  200,  300,  400,  500,  600,  700,  800,  900, 1000])},\n",
       "             scoring=&#x27;r2&#x27;, verbose=10)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-52\" type=\"checkbox\" ><label for=\"sk-estimator-id-52\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-53\" type=\"checkbox\" ><label for=\"sk-estimator-id-53\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestRegressor(), n_jobs=-1,\n",
       "             param_grid={'max_depth': [10, 12, 15],\n",
       "                         'n_estimators': array([ 100,  200,  300,  400,  500,  600,  700,  800,  900, 1000])},\n",
       "             scoring='r2', verbose=10)"
      ]
     },
     "execution_count": 581,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_top_6_CV=np.array(X_train_top_6_CV)\n",
    "y_train_top_6_CV=np.array(y_train_top_6_CV)\n",
    "\n",
    "# Import GridSearch for hyperparameter tuning using Cross-Validation (CV)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the Hyper-parameter Grid to search on, in case of Random Forest\n",
    "param_grid = {'n_estimators': np.arange(100,1001,100),   # number of trees 100, 200, ..., 1000\n",
    "              'max_depth': [10,12,15], # depth of trees 10, 11, ..., 20\n",
    "              }             \n",
    "\n",
    "# Create the Hyper-parameter Grid\n",
    "hpGrid = GridSearchCV(RandomForestRegressor(),   # the model family\n",
    "                      param_grid,                 # the search grid\n",
    "                      cv = 5,                     # 5-fold cross-validation\n",
    "                      scoring = 'r2',\n",
    "                      n_jobs=-1,\n",
    "                      verbose=10)       # score to evaluate\n",
    "  \n",
    "# Train the models using Cross-Validation\n",
    "hpGrid.fit(X_train_top_6_CV, y_train_top_6_CV.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestRegressor(max_depth=15, n_estimators=1000)\n",
      "0.611477370912186\n"
     ]
    }
   ],
   "source": [
    "# Fetch the best Model or the best set of Hyper-parameters\n",
    "print(hpGrid.best_estimator_)\n",
    "\n",
    "# Print the score (accuracy) of the best Model after CV\n",
    "print(np.abs(hpGrid.best_score_))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we obtained a set of parameters using gridsearch, let's build our improved model. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Improved Model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 469281 entries, 0 to 469280\n",
      "Data columns (total 17 columns):\n",
      " #   Column            Non-Null Count   Dtype  \n",
      "---  ------            --------------   -----  \n",
      " 0   popularity        469281 non-null  int64  \n",
      " 1   duration_ms       469281 non-null  int64  \n",
      " 2   explicit          469281 non-null  int64  \n",
      " 3   danceability      469281 non-null  float64\n",
      " 4   energy            469281 non-null  float64\n",
      " 5   key               469281 non-null  int64  \n",
      " 6   loudness          469281 non-null  float64\n",
      " 7   mode              469281 non-null  int64  \n",
      " 8   speechiness       469281 non-null  float64\n",
      " 9   acousticness      469281 non-null  float64\n",
      " 10  instrumentalness  469281 non-null  float64\n",
      " 11  liveness          469281 non-null  float64\n",
      " 12  valence           469281 non-null  float64\n",
      " 13  tempo             469281 non-null  float64\n",
      " 14  time_signature    469281 non-null  int64  \n",
      " 15  num_artists       469281 non-null  int64  \n",
      " 16  year              469281 non-null  int64  \n",
      "dtypes: float64(9), int64(8)\n",
      "memory usage: 60.9 MB\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset into a pandas dataframe\n",
    "RefinedMusicData = pd.read_csv('../Data/tracks_cleaned.csv')\n",
    "RefinedMusicData.drop('release_date',axis=1,inplace=True)\n",
    "RefinedMusicData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning and arrangement\n",
    "RefinedMusicData.drop(['key'],axis=1, inplace=True)\n",
    "time_signature_df=pd.get_dummies(RefinedMusicData[\"time_signature\"])\n",
    "time_signature_df.columns = time_signature_df.columns.astype(str)\n",
    "RefinedMusicData = pd.concat([RefinedMusicData,time_signature_df],axis=1)\n",
    "RefinedMusicData['mode'] = np.where(RefinedMusicData['mode']=='Major', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 469281 entries, 0 to 469280\n",
      "Data columns (total 20 columns):\n",
      " #   Column            Non-Null Count   Dtype  \n",
      "---  ------            --------------   -----  \n",
      " 0   duration_ms       469281 non-null  int64  \n",
      " 1   explicit          469281 non-null  int64  \n",
      " 2   danceability      469281 non-null  float64\n",
      " 3   energy            469281 non-null  float64\n",
      " 4   loudness          469281 non-null  float64\n",
      " 5   mode              469281 non-null  int64  \n",
      " 6   speechiness       469281 non-null  float64\n",
      " 7   acousticness      469281 non-null  float64\n",
      " 8   instrumentalness  469281 non-null  float64\n",
      " 9   liveness          469281 non-null  float64\n",
      " 10  valence           469281 non-null  float64\n",
      " 11  tempo             469281 non-null  float64\n",
      " 12  time_signature    469281 non-null  int64  \n",
      " 13  num_artists       469281 non-null  int64  \n",
      " 14  year              469281 non-null  int64  \n",
      " 15  0                 469281 non-null  uint8  \n",
      " 16  1                 469281 non-null  uint8  \n",
      " 17  3                 469281 non-null  uint8  \n",
      " 18  4                 469281 non-null  uint8  \n",
      " 19  5                 469281 non-null  uint8  \n",
      "dtypes: float64(9), int64(6), uint8(5)\n",
      "memory usage: 55.9 MB\n"
     ]
    }
   ],
   "source": [
    "# data modelling\n",
    "X_improved= RefinedMusicData.loc[:,RefinedMusicData.columns !=\"popularity\"] # all the features accept popularity\n",
    "y_improved = RefinedMusicData[\"popularity\"] # the popularity\n",
    "X_improved.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting\n",
    "\n",
    "# separate the data to training and testing\n",
    "X_train_improved, X_test_improved, y_train_improved,y_test_improved=train_test_split(X_improved,y_improved,test_size=0.2)\n",
    "\n",
    "# save as np.array\n",
    "X_test_improved = np.array(X_test_improved)\n",
    "y_test_improved = np.array(y_test_improved)\n",
    "X_train_improved=np.array(X_train_improved)\n",
    "y_train_improved=np.array(y_train_improved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(max_depth=15, n_estimators=1000, n_jobs=-1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_depth=15, n_estimators=1000, n_jobs=-1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(max_depth=15, n_estimators=1000, n_jobs=-1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random forest regressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model_random_forest_improved = RandomForestRegressor(n_estimators=1000, max_depth=15,n_jobs=-1)\n",
    "model_random_forest_improved.fit(X_train_improved,y_train_improved)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'model_random_forest_improved' (RandomForestRegressor)\n"
     ]
    }
   ],
   "source": [
    "# to import to another jupyter notebook\n",
    "%store model_random_forest_improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Performance\n",
      "R^2: 0.5981622323851397\n",
      "MSE:  135.29050696919228\n",
      "RMSE: 11.631444749866299\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Set Performance\")\n",
    "print(\"R^2: \" + str(model_random_forest_improved.score(X_train_improved,y_train_improved)))\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Predict on the train set\n",
    "y_train_pred = model_random_forest_improved.predict(X_train_improved)\n",
    "\n",
    "# Calculate MSE and RMSE on train set\n",
    "mse_train = mean_squared_error(y_train_improved, y_train_pred)\n",
    "\n",
    "rmse_train = np.sqrt(mse_train)\n",
    "print(\"MSE: \", mse_train)\n",
    "print(\"RMSE:\", rmse_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Performance\n",
      "R^2: 0.5103836840658821\n",
      "MSE:  166.41431142653403\n",
      "RMSE: 12.900167108473209\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Set Performance\")\n",
    "print(\"R^2: \" + str(model_random_forest_improved.score(X_test_improved,y_test_improved)))\n",
    "\n",
    "\n",
    "# Predict on the train set\n",
    "y_test_pred = model_random_forest_improved.predict(X_test_improved)\n",
    "\n",
    "# Calculate MSE and RMSE on train set\n",
    "mse_test = mean_squared_error(y_test_improved, y_test_pred)\n",
    "\n",
    "rmse_test = np.sqrt(mse_test)\n",
    "print(\"MSE: \", mse_test)\n",
    "print(\"RMSE:\", rmse_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Conclusion\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, we can observe that after fine tuning the model, we have seen an improved R^2 value as well as a reduced MSE and RMSE value. \n",
    "\n",
    "The model can be improved further but requires much more computational power. In our fine tuning of model we considered only the top 6 non-negligible factors. However, given more computational powers we could include all the features such that the random forests could more accurately determine the appropriate weight.\n",
    "\n",
    "In addition, we could better sample our data to reduce the class imbalance. However, this requires much more computational power as well. \n",
    "\n",
    "Nonetheless, seeing an improvement in R^2 value for test data from `0.47486637863325565`to `0.5111782273071137` shows that the model could be further optimized. However, this would require more computational power."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
